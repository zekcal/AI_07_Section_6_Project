{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석을 위한 tokenize\n",
    "1단계를 통해 기초적인 전처리만을 거친 raw data를 txt 파일로 확보하였다. 분석과 활용을 위해 토큰화를 실시하였다.  \n",
    "다양한 라이브러리 가운데, 트윗 데이터에선 줄임말이나 축약어가 많이 나올 것이라 생각해 비지도학습이 사용된 [soynlp](https://github.com/lovit/soynlp)를 사용하였다.\n",
    "셋 모두 같은 과정을 거치기 때문에 함수나 py파일로 작성해 실행하는 것도 방법이지만, 하나씩 과정을 보이고자 풀어 진행하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install soynlp를 통해 설치함\n",
    "from soynlp.noun import LRNounExtractor\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "import re #정규화 처리를 위해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 성경에 대한 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentences of bible = 31116\n"
     ]
    }
   ],
   "source": [
    "#변수에 이름 저장\n",
    "edited_bible = 'edited_bible.txt'\n",
    "#raw의 문장을 리스트 안에 넣음\n",
    "with open(edited_bible, encoding='cp949') as f:\n",
    "    bible_sentences = [re.sub(r'[\\n]','',doc) for doc in f] \n",
    "print('num sentences of bible = %d' % len(bible_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.284 Gbory 0.250 Gb\n",
      "all cohesion probabilities was computed. # words = 26435\n",
      "all branching entropies was computed # words = 42652\n",
      "all accessor variety was computed # words = 42652\n"
     ]
    }
   ],
   "source": [
    "#raw를 학습해서 사전 테이블에 넣기\n",
    "bible_word_extractor = WordExtractor()\n",
    "bible_word_extractor.train(bible_sentences)\n",
    "bible_word_score_table = bible_word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['태초에', '하나님', '이', '천지를', '창조하', '시니라']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#토크나이저를 적용함\n",
    "bible_scores = {word:score.cohesion_forward for word, score in bible_word_score_table.items()}\n",
    "bible_l_tokenizer = LTokenizer(scores=bible_scores)\n",
    "#리스트에 담아서, 성경엔 없으나 중복되는 문구(ex: ㅋㅋㅋ와 ㅋㅋㅋㅋ는 동일함)를 제거해서 토큰화함\n",
    "bible_tokenized = [bible_l_tokenizer.tokenize((repeat_normalize(i, num_repeats=3)), flatten=True) for i in bible_sentences]\n",
    "bible_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 판결문에 대한 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentences of judgement = 24329\n"
     ]
    }
   ],
   "source": [
    "#변수에 이름 저장\n",
    "edited_judgement = 'edited_judgement.txt'\n",
    "#raw의 문장을 리스트 안에 넣음\n",
    "with open(edited_judgement, encoding='cp949') as f:\n",
    "    judge_sentences = [re.sub('[,.\\n]','',doc) for doc in f] \n",
    "print('num sentences of judgement = %d' % len(judge_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.858 Gbory 0.745 Gb\n",
      "all cohesion probabilities was computed. # words = 104816\n",
      "all branching entropies was computed # words = 177672\n",
      "all accessor variety was computed # words = 177672\n"
     ]
    }
   ],
   "source": [
    "#raw를 학습해서 사전 테이블에 넣기\n",
    "judge_word_extractor = WordExtractor()\n",
    "judge_word_extractor.train(judge_sentences)\n",
    "judge_word_score_table = judge_word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['원고가',\n",
       " '소속회사',\n",
       " '의',\n",
       " '노동조합',\n",
       " '에서',\n",
       " '분규가',\n",
       " '발생',\n",
       " '하자',\n",
       " '노조활동',\n",
       " '을',\n",
       " '구실로',\n",
       " '정상적인',\n",
       " '근무',\n",
       " '를',\n",
       " '해태하고',\n",
       " '노조조합장이',\n",
       " '사임한',\n",
       " '경우',\n",
       " '노동조합',\n",
       " '규약에',\n",
       " '동',\n",
       " '조합',\n",
       " '장의',\n",
       " '직무',\n",
       " '를',\n",
       " '대행할',\n",
       " '자를',\n",
       " '규정',\n",
       " '해',\n",
       " '두고',\n",
       " '있음에도',\n",
       " '원고',\n",
       " '자신이',\n",
       " '주동하여',\n",
       " '노조자치수습대책위원회를',\n",
       " '구성',\n",
       " '하여',\n",
       " '그',\n",
       " '위원장으로',\n",
       " '피선되어',\n",
       " '근무시간',\n",
       " '중에도',\n",
       " '노조활동',\n",
       " '을',\n",
       " '벌여',\n",
       " '운수업체인',\n",
       " '소속회사',\n",
       " '의',\n",
       " '업무',\n",
       " '에',\n",
       " '지장을',\n",
       " '초래하',\n",
       " '고',\n",
       " '종업원들에게',\n",
       " '도',\n",
       " '나쁜',\n",
       " '영향을',\n",
       " '끼쳐',\n",
       " '소속회사',\n",
       " '가',\n",
       " '취업규칙',\n",
       " '을',\n",
       " '위반하',\n",
       " '고',\n",
       " '고의로',\n",
       " '회사',\n",
       " '업무능률을',\n",
       " '저해하',\n",
       " '였으며',\n",
       " '회사',\n",
       " '업무상의',\n",
       " '지휘명령',\n",
       " '에',\n",
       " '위반하였음을',\n",
       " '이유로',\n",
       " '원고',\n",
       " '를',\n",
       " '징계',\n",
       " '해고',\n",
       " '하였다면',\n",
       " '이는',\n",
       " '원고',\n",
       " '의',\n",
       " '노동조합',\n",
       " '활동',\n",
       " '과는',\n",
       " '관계없이',\n",
       " '회사',\n",
       " '취업규칙에',\n",
       " '의하여',\n",
       " '사내질서를',\n",
       " '유지하기',\n",
       " '위한',\n",
       " '사용자',\n",
       " '고유의',\n",
       " '징계',\n",
       " '권에',\n",
       " '기하여',\n",
       " '이루어진',\n",
       " '정당한',\n",
       " '징계',\n",
       " '권의',\n",
       " '행사',\n",
       " '로',\n",
       " '보아야',\n",
       " '한다']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#토크나이저를 적용함\n",
    "judge_scores = {word:score.cohesion_forward for word, score in judge_word_score_table.items()}\n",
    "judge_l_tokenizer = LTokenizer(scores=judge_scores)\n",
    "#리스트에 담아서, 판결문엔 없으나 중복되는 문구(ex: ㅋㅋㅋ와 ㅋㅋㅋㅋ는 동일함)를 제거해서 토큰화함\n",
    "judge_tokenized = [judge_l_tokenizer.tokenize((repeat_normalize(i, num_repeats=3)), flatten=True) for i in judge_sentences]\n",
    "judge_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 트윗에 대한 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentences of twit = 14907\n"
     ]
    }
   ],
   "source": [
    "#변수에 이름 저장\n",
    "edited_twit = 'edited_twit.txt'\n",
    "#raw의 문장을 리스트 안에 넣음\n",
    "with open(edited_twit, encoding='utf-8') as f: #이건 이상하게 인코딩이 다름\n",
    "    twit_sentences = [re.sub('[/,\\n[\\]]','',doc) for doc in f] \n",
    "print('num sentences of twit = %d' % len(twit_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.886 Gbory 0.886 Gb\n",
      "all cohesion probabilities was computed. # words = 8277\n",
      "all branching entropies was computed # words = 14428\n",
      "all accessor variety was computed # words = 14428\n"
     ]
    }
   ],
   "source": [
    "#raw를 학습해서 사전 테이블에 넣기\n",
    "twit_word_extractor = WordExtractor()\n",
    "twit_word_extractor.train(twit_sentences)\n",
    "twit_word_score_table = twit_word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['최민정',\n",
       " '선수',\n",
       " '어케',\n",
       " '저럼...',\n",
       " '세계',\n",
       " '신기록',\n",
       " '올림픽',\n",
       " '신기록',\n",
       " '둘',\n",
       " '다',\n",
       " '가진',\n",
       " '여자',\n",
       " '어때',\n",
       " 'ㄴ',\n",
       " '갖고',\n",
       " '싶어...']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#토크나이저를 적용함\n",
    "twit_scores = {word:score.cohesion_forward for word, score in twit_word_score_table.items()}\n",
    "twit_l_tokenizer = LTokenizer(scores=twit_scores)\n",
    "#리스트에 담아서, 중복되는 문구(ex: ㅋㅋㅋ와 ㅋㅋㅋㅋ는 동일함)를 제거해서 토큰화함\n",
    "twit_tokenized = [twit_l_tokenizer.tokenize((repeat_normalize(i, num_repeats=3)), flatten=True) for i in twit_sentences]\n",
    "twit_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 토큰화 끝난 것들을 txt로 저장\n",
    "딥 러닝에 적용하기 위해 우선 파일 형태로 저장하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#성경\n",
    "with open('tokenized_bible.txt', 'w', encoding='UTF8') as f:\n",
    "    for line in bible_tokenized:\n",
    "        f.write(str(line) + '\\n')\n",
    "\n",
    "#판결문\n",
    "with open('tokenized_judge.txt', 'w', encoding='UTF8') as f:\n",
    "    for line in judge_tokenized:\n",
    "        f.write(str(line) + '\\n')\n",
    "        \n",
    "#트윗\n",
    "with open('tokenized_twit.txt', 'w', encoding='UTF8') as f:\n",
    "    for line in twit_tokenized:\n",
    "        f.write(str(line) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb5c5580cbdc7edaf1dbcbae550fc6e32a20e6d30e58e8d8249ea42c2b8c8784"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('project1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
