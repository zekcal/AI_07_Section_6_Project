{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KOGPT2를 활용한 학습 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리 불러오기, pip로 설치하였음\n",
    "import gluonnlp as nlp #kogpt2에서 사용했어서 사용\n",
    "from gluonnlp.data import SentencepieceTokenizer #상동\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 생성 방식 따라 디렉토리 경로 입력하는 클래스\n",
    "class GPT2Model(tf.keras.Model):\n",
    "    def __init__(self, dir_path):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.gpt2(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#링크에서 사전학습된 모델을 다운받는다\n",
    "'''\n",
    "https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#모델 위치 설정 및 불러오기\n",
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "gpt_model = GPT2Model(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사전학습 모델 구성하기\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "MAX_LEN = 40\n",
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, num_best=0, alpha=0)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                               mask_token=None,\n",
    "                                               sep_token=None,\n",
    "                                               cls_token=None,\n",
    "                                               unknown_token='<unk>',\n",
    "                                               padding_token='<pad>',\n",
    "                                               bos_token='<s>',\n",
    "                                               eos_token='</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저 & 사전 학습 모델 통해 문장 만들기 함수\n",
    "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
    "    _logits = logits.numpy()\n",
    "    top_k = min(top_k, logits.shape[-1])  \n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
    "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
    "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
    "        \n",
    "        _logits[indices_to_remove] = filter_value\n",
    "    return tf.constant([_logits])\n",
    "\n",
    "#seed_word:시작하는 단어\n",
    "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
    "    sent = seed_word\n",
    "    toked = tokenizer(sent)\n",
    "    \n",
    "    for _ in range(max_step):\n",
    "        #인풋 = 토큰화된 단어의 인덱스\n",
    "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
    "        #아웃풋 = 문장의 마지막 단어\n",
    "        outputs = model(input_ids)[:, -1, :]\n",
    "        if greedy: #가장 확률 높은 단어만 선택해서 반복되는 경우가 잦음\n",
    "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
    "        else: #그렇지 않음\n",
    "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
    "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
    "        if gen == '</s>': #멈춤단어나오면 멈춤\n",
    "            break\n",
    "        sent += gen.replace('▁', ' ')\n",
    "        toked = tokenizer(sent)\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금 델따 줄<unk>~◈ 스티브W.\n",
      "고양인우리에서 일할 광교신도시 입주민들로 구성된 ‘인더스밸리3단지’(현대산업개발, 시공사, 삼성물산)다.\n",
      "기분이 좋아졌어요”라며 환하게 미소 지었다.\n"
     ]
    }
   ],
   "source": [
    "#사전 학습된 모델 확인\n",
    "print(generate_sent('지금', gpt_model))\n",
    "print(generate_sent('고양인', gpt_model))\n",
    "print(generate_sent('기분이 좋아', gpt_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#성경 데이터 넣기\n",
    "sents = [s[:-1] for s in open('edited_bible.txt', encoding='cp949').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저에 인풋 아웃풋 나눠 넣기\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for s in sents:\n",
    "    tokens = [vocab[vocab.bos_token],]  + vocab[tokenizer(s)] + [vocab[vocab.eos_token],]\n",
    "    input_data.append(tokens[:-1])\n",
    "    output_data.append(tokens[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#패딩 해서 입출력 구성\n",
    "input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "\n",
    "input_data = np.array(input_data, dtype=np.int64)\n",
    "output_data = np.array(output_data, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수 & 정확도 측정 설정\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask    \n",
    "    acc = train_accuracy(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 컴파일\n",
    "gpt_model.compile(loss=loss_function,\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[accuracy_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1945/1945 [==============================] - 4149s 2s/step - loss: 2.5410 - accuracy_function: 0.1838\n",
      "Epoch 2/5\n",
      "1945/1945 [==============================] - 4369s 2s/step - loss: 2.2639 - accuracy_function: 0.2067\n",
      "Epoch 3/5\n",
      "1945/1945 [==============================] - 3912s 2s/step - loss: 2.1013 - accuracy_function: 0.2197\n",
      "Epoch 4/5\n",
      "1945/1945 [==============================] - 3907s 2s/step - loss: 1.9717 - accuracy_function: 0.2304\n",
      "Epoch 5/5\n",
      "1945/1945 [==============================] - 3907s 2s/step - loss: 1.8556 - accuracy_function: 0.2400\n"
     ]
    }
   ],
   "source": [
    "#학습 실행\n",
    "history = gpt_model.fit(input_data, output_data, \n",
    "                    batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./data_out\\bible_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#저장\n",
    "import os\n",
    "DATA_OUT_PATH = './data_out'\n",
    "model_name = 'bible_model'\n",
    "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "gpt_model.gpt2.save_pretrained(save_path)\n",
    "\n",
    "loaded_gpt_model = GPT2Model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로젝트 시험 참가자들은 다 여로함 같아 오늘 내가 너희에게 한 것을 생각하되 너희에게 말하노니 너희 가운데서 여인이 머리털로 자랐으므로 여인이 되었도다 하고\n",
      "--------------------------------\n",
      "마라탕 존맛탱은 끓는 가루를 밀가루 반죽에 사르는 중이요 온천 깊은 곳까지 드리는 중이요 거룩한 곳까지 드리는 중이요 여호와를 위한\n",
      "--------------------------------\n",
      "어젯밤은 유난히 쌀쌀하여 버티기 어려웠다.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(generate_sent('프로젝트', gpt_model))\n",
    "print('-'*32)\n",
    "print(generate_sent('마라탕 존맛탱', gpt_model))\n",
    "print('-'*32)\n",
    "print(generate_sent('어젯밤은 유난히 쌀쌀하여 버티기 어려웠다.', gpt_model))\n",
    "print('-'*32)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb5c5580cbdc7edaf1dbcbae550fc6e32a20e6d30e58e8d8249ea42c2b8c8784"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('project1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
